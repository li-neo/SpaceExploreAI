{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4831806-a5d3-48da-b8aa-4930049480a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'array'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     80\u001b[39m             \u001b[38;5;28mself\u001b[39m.bias1 += np.sum(hidden_delta, axis=\u001b[32m0\u001b[39m) * lr\n\u001b[32m     83\u001b[39m              \u001b[38;5;66;03m# 每一百轮打印一次损失\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m             \u001b[38;5;66;03m# if epoch % 200 == 0:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m#   取异或\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m x_input = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m([[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m], [\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m]])\n\u001b[32m     98\u001b[39m y_output = np.array([[\u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m], [\u001b[32m1\u001b[39m], [\u001b[32m0\u001b[39m]])\n\u001b[32m    100\u001b[39m nn = SimpleNeuralNetwork(input_size=\u001b[32m2\u001b[39m, hidden_size=\u001b[32m2\u001b[39m, output_size=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'array'"
     ]
    }
   ],
   "source": [
    "# %load SimpleNeuralNetwork.py\n",
    "import numpy as np\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        # 初始化权重矩阵，随机小数\n",
    "\n",
    "        #\n",
    "        #     input:     x1     x2\n",
    "        #     hidden:    h1     h2\n",
    "        #     output:        y\n",
    "        #\n",
    "\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size)\n",
    "        self.bias1 = np.random.randn(hidden_size)\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias2 = np.random.randn(output_size)\n",
    "     #  激活函数 \n",
    "     # 激活函数\t 输出范围\t导数特性\t适用场景\n",
    "     #  Sigmoid\t(0, 1)\t导数易消失\t二分类输出层\n",
    "     #  Tanh\t(-1, 1)\t梯度比 Sigmoid 更强\t隐藏层\n",
    "     #  ReLU\t[0, +∞)\t简单且缓解梯度消失\t隐藏层（最常用）\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # sigmod 激活函数的导数\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    # 前向网络\n",
    "    def  forward(self, x):\n",
    "        # 第一层： 输入层 -> 隐藏层\n",
    "        self.hidden_input = np.dot(x, self.weights1) + self.bias1\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "\n",
    "        # 第二层： 隐藏层  -> 输出层\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights2) +  self.bias2\n",
    "        self.output = self.sigmoid(self.output_input)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    # 训练\n",
    "    # x: 输入参数\n",
    "    # y: 输出\n",
    "    # epochs: 训练次数\n",
    "    # lr: 学习因子\n",
    "    def train(self, x, y, epochs = 10000, lr = 0.1):\n",
    "        for epoch in range(epochs):\n",
    "            # 前向网络传播计算\n",
    "\n",
    "            output = self.forward(x)\n",
    "\n",
    "            # 反向网络传播计算， 梯度下降\n",
    "            output_error = y - output\n",
    "            # 输出层 -> 隐藏层\n",
    "            # 输出层delta\n",
    "            output_delta = output_error * self.sigmoid_derivative(output)\n",
    "\n",
    "            #计算隐藏层error, delta\n",
    "            hidden_error = output_delta.dot(self.weights2.T)\n",
    "            hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "            # 更新权重和偏置\n",
    "\n",
    "            self.weights2 += self.hidden_output.T.dot(output_delta) * lr\n",
    "\n",
    "            # ​数学原理：\n",
    "            # 偏置的梯度是输出层误差在各样本上的和（因偏置对每个样本的误差均有贡献）：\n",
    "            # np.sum(output_delta, axis=0)：沿样本维度（axis=0）求和，得到形状 [1, n_output]。\n",
    "            # keepdims=True：保持维度，确保与 bias2 的维度一致。\n",
    "            # * lr：乘以学习率。\n",
    "            self.bias2 += np.sum(output_delta, axis=0) * lr\n",
    "            # self.bias2 += np.sum(output_delta, axis=0, keepdims=True) * lr\n",
    "\n",
    "            #更新输入层和隐藏层的权重和偏置\n",
    "\n",
    "            self.weights1 += x.T.dot(hidden_delta) * lr\n",
    "            self.bias1 += np.sum(hidden_delta, axis=0) * lr\n",
    "             \n",
    "\n",
    "             # 每一百轮打印一次损失\n",
    "\n",
    "            # if epoch % 200 == 0:\n",
    "                #                 计算均方误差损失**\n",
    "                # loss = np.mean(np.square(y - output))\n",
    "                # ​步骤拆解:\n",
    "                # y - output：计算每个样本的预测值与真实值的差值（形状与 y 和 output 相同）。\n",
    "                # np.square(...)：对差值逐元素平方，放大较大误差（如误差为2 → 平方后为4）。\n",
    "                # np.mean(...)：对所有样本的平方误差取平均，得到标量损失值\n",
    "\n",
    "                # loss = np.mean(np.square(y - output))\n",
    "                # print(f\"Epoch:{epoch}, Loss:{loss:.4f}\")\n",
    "\n",
    "#   取异或\n",
    "x_input = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y_output = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "nn.train(x_input, y_output, epochs=10000, lr=0.15)\n",
    "\n",
    "predictions = nn.forward(x_input)\n",
    "\n",
    "print(f\"\\nPredictions:\\n{predictions}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b823e43-632d-49ae-b0fe-aec66b91b1b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %load SimpleNNTensorflow.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# %load SimpleNNTensorflow.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 数据准备\n",
    "X = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
    "y = tf.constant([[0], [1], [1], [0]], dtype=tf.float32)\n",
    "\n",
    "# 定义模型结构\n",
    "model = Sequential([\n",
    "    Dense(2, input_dim=2, activation='sigmoid'),  # 隐藏层（2个神经元）\n",
    "    Dense(1, activation='sigmoid')               # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X, y, epochs=10000, verbose=0)\n",
    "\n",
    "# 测试模型\n",
    "predictions = model.predict(X)\n",
    "print(\"预测结果:\", predictions.round().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdf1f0-0201-4b69-9e51-c6fcbf94c176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
